{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src='images/besm.png' width='250px'> -->\n",
    "<style>\n",
    "    \n",
    "@font-face {font-family: \"B Lotus\"; src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot\"); src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.svg#B Lotus\") format(\"svg\"); }\n",
    "\n",
    "\n",
    "</style>\n",
    "\n",
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>بسم الله الرحمن الرحیم </center> \n",
    "</h1> \n",
    "\n",
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>طراحی سیستم بازیابی اطلاعات </center> \n",
    "</h1> \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "    <center> Contact: Amin Saeidi - amin.saeidi.1997@gmail.com - 400211579 </center>\n",
    "    <center> Contact: Zahra Fazli  -z.fazli124@yahoo.com - 401301196 </center>\n",
    "    <center> Contact: Hamid Reza Akbari - hakbari@gmail.com - 401301543</center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>هدف و تعریف مسئله </center> \n",
    "</h1> \n",
    "\n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "هدف این مساله پیدا کردن اسناد مرتبط با کویری و صحت سنجی نتایج به دست آمده می‌باشد.  این کار به سه روش مختلف پیاده سازی شده است. در روش اول با استفاده از tf-idf اسناد جاسازی شده‌اند و کوئری روی آن‌ها اعمال شده است. در روش دوم این جاسازی به کمک برت فارسی انجام شده است و در روش سوم، به جای میانگین گرفتن روی جاسازی تمام کلمات، به کمک tf-idf، میانگین وزن دار روی جاسازی‌های برت انجام گرفته تا به جاسازی سند برسیم. در ادامه مراحل کار و نتایج توضیح داده می‌شود.\n",
    "</div>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>نصب موارد مورد نیاز و فراخوانی</center> \n",
    "</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25961,
     "status": "ok",
     "timestamp": 1673748016866,
     "user": {
      "displayName": "z fazli",
      "userId": "01348446294497098456"
     },
     "user_tz": -210
    },
    "id": "Y-g8KXdOTQpg",
    "outputId": "2fc80703-815a-4e60-a773-0cd67fa22a97"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CWmVuIn2MTuC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import random as rand\n",
    "from hazm import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer , PreTrainedTokenizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>خواندن اسناد و کويری‌ها و پیش پردازش  </center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 16px'>\n",
    "  <ol>\n",
    "      <li>تابع (pre_processing_query): کارهای پیش‌پردازش اعم از نرمال سازی متن و ریشه یابی روی کوئری را انجام‌ می‌دهد.</li>\n",
    "      <li>تابع (tokenize_normalize_lemmatize):  برای تمام اسناد خوانده شده کارهای پیش‌پردازشی مشتمل بر نرمال سازی متن و ریشه یابی را انجام می‌دهد. با توجه به نوع مسئله ریشه یابی کلمات در یافتن عبارت های مشابه موثر خواهد بود</li>\n",
    "      <li>تابع (load_documents): فایل‌های موجود در مسیر برنامه را به دست آورده و لیست آنها را برای خواندن به تابع load_document_list می‌دهد. فرمت اطلاعات مشتمل بر کل متن، لیست جملات و لیست کلمات متن در نظر گرفته شده است</li>\n",
    "      <li>تابع (load_documents_list): لیست فایل‌های داده شده را به ترتیب می‌خواند و همه‌ي اطلاعات خوانده شده را برمی‌گرداند</li>\n",
    "      <li>تابع (load_evaluation_file): با توجه به فرمت فایل ارزیابی ارائه شده، محتوای مورد نظر را به کمک تابع extract_evaluation_data اطلاعات لازم را استخراج می‌کند. </li>\n",
    "      <li>تابع (extract_evaluation_data): اطلاعات موجود در فایل ارزیابی را استخراج می‌کند</li>\n",
    "      <li>تابع (find_end_query_index):بر اساس یک کوئری انتهای آن را جهت بررسی کلمات فیمابین پیدا می‌کند. </li>\n",
    "      <li>تابع (is_token_query): با توجه به فرمت اطلاعات ارزیابی برای تعیین شروع یک کوئری از این تابع استفاده می کند. </li>\n",
    "\n",
    "  </ol> \n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TB5Cc432hskq"
   },
   "outputs": [],
   "source": [
    "class LoadAndPreprocessing:\n",
    "    path_data = './IR_dataset'\n",
    "    evaluation_data = 'evaluation_IR.yml'\n",
    "    evaluation_reserve_word = ['relevant', 'similar_high', 'similar_low', 'similar_med']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.normalizer = Normalizer()\n",
    "        self.lemmatizer = Lemmatizer()\n",
    "\n",
    "    def pre_processing_query(self, query):\n",
    "        query = self.normalizer.normalize(query)\n",
    "        words_list = [self.lemmatizer.lemmatize(w) for w in word_tokenize(query)]\n",
    "        return ' '.join(words_list)\n",
    "\n",
    "    def tokenize_normalize_lemmatize(self, data):\n",
    "        for filename in tqdm.tqdm(data):\n",
    "            x = data[filename]\n",
    "            text = x['content']\n",
    "            text = self.normalizer.normalize(text)\n",
    "\n",
    "            sent_list = sent_tokenize(text)\n",
    "            x['sentences'] = sent_list\n",
    "\n",
    "            for sent in sent_list:\n",
    "                x['words'] = [[self.lemmatizer.lemmatize(w)] for w in word_tokenize(sent)]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def load_documents(self):\n",
    "        training_docs = [f for f in listdir(self.path_data) if isfile(join(self.path_data, f))]\n",
    "        training_data = self.load_documents_list(training_docs)\n",
    "        return training_data\n",
    "\n",
    "    def load_documents_list(self, filename_list):\n",
    "        data = {}\n",
    "        for file_name in tqdm.tqdm(filename_list):\n",
    "            text = codecs.open(f\"{self.path_data}/{file_name}\", 'r', 'utf-8').read()\n",
    "            data[file_name] = {'content': text}\n",
    "\n",
    "        return data\n",
    "\n",
    "    def load_evaluation_file(self):\n",
    "        lines = codecs.open(self.evaluation_data, 'r', 'utf-8').readlines()\n",
    "        token_list = []\n",
    "        for line in lines:\n",
    "            line = line.replace(':', '')\n",
    "            line = line.replace('-', '')\n",
    "            line = line.strip()\n",
    "            token_list.append(line)\n",
    "\n",
    "        data = {}\n",
    "        start_query_index = 0\n",
    "        while start_query_index < len(token_list):\n",
    "            query = token_list[start_query_index]\n",
    "            end_query_index = self.find_end_query_index(token_list, start_query_index)\n",
    "            data_query = self.extract_evaluation_data(token_list[start_query_index + 1:end_query_index])\n",
    "            query = self.pre_processing_query(query)\n",
    "            data[query] = data_query\n",
    "            start_query_index = end_query_index + 1\n",
    "\n",
    "        return data\n",
    "\n",
    "    def extract_evaluation_data(self, token_list):\n",
    "        data = {}\n",
    "        last_label = token_list[0]\n",
    "        docs_list = []\n",
    "        for index in range(1, len(token_list)):\n",
    "            if token_list[index] in self.evaluation_reserve_word:\n",
    "                data[last_label] = docs_list\n",
    "                last_label = token_list[index]\n",
    "                docs_list = []\n",
    "                continue\n",
    "\n",
    "            docs_list.append(token_list[index])\n",
    "\n",
    "        data[last_label] = docs_list\n",
    "        return data\n",
    "\n",
    "    def find_end_query_index(self, token_list, start_query_index):\n",
    "        end_query_index = start_query_index + 1\n",
    "        while end_query_index < len(token_list):\n",
    "            if self.is_token_query(token_list[end_query_index]):\n",
    "                return end_query_index - 1\n",
    "            end_query_index += 1\n",
    "\n",
    "        return len(token_list) + 1\n",
    "\n",
    "    def is_token_query(self, token):\n",
    "        if token.isnumeric():\n",
    "            return False\n",
    "        if token in self.evaluation_reserve_word:\n",
    "            return False\n",
    "        if token in [':', '-']:\n",
    "            return False\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'> قسمت اول - tf/idf</center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 16px'>\n",
    "  <ol>\n",
    "      <li>تابع (set_tfidf_word): به ازای هر کلمه مقدار tfidf آن را به دست میاورد. (مورد استفاده در قسمت سوم)</li>\n",
    "      <li>تابع (token_to_tfidf): از آنجایی که برت با آی دی توکن‌ها کار می‌کند، یک دیکشنری از آی دی به مقدار tfidf می‌سازیم.( مورد استفاده در قسمت سوم) </li>\n",
    "      <li>تابع (predict): تابع اصلی جستجوی اسناد مبتنی بر همه کوئری ها در این تابع انجام و نتیجه جستجو در این بخش گزارش می شود. توجه به این نکته ضروری است که در صورتیکه یکی از مستندات یافت شده در مجموع داده های ارزیابی مدل وجود داشته باشد عملیات جستجو بر روی مستند مورد نظر با موفقیت گزارش می شود.. اما برای درک بهتر دقت عملیات این تفضیل به تفکیک میزان شباهت مستندات نیز گزارش شده است. عموما دقت تفضیل از دقت کلی کمتر می باشد</li>\n",
    "      <li>تابع (predict_query):بر اساس ماتریس شباهت که با روش TF-IDF تولید شده اندیس مستندات مربوطه را از ماتریس استخراج و سپس نام هریک از فایل های بازیابی شده را برای مقایسه در بخش ارزیابی ارائه می نماید  </li>\n",
    "      <li>تابع (create_corpus): داده‌ی ورودی (محتوای فایل‌ها ) را به قالب لیست متن و لیست نام مستندات تهیه و ارائه می نماید </li>\n",
    "      <li>تابع (largest_indices): در صورتیکه در پارامتر ورودی مقدار n تعیین شود تعداد n اندیس آرایه با بیشترین مقدار را که به صورت نزولی مرتب شده است برمی گرداند.</li>\n",
    "\n",
    "  </ol> \n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-4mbQX6yjVMg"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "class TfIdfVectorization:\n",
    "    path = 'data'\n",
    "\n",
    "    def __init__(self, number):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.number = number\n",
    "\n",
    "    def set_tfidf_word( self , training_data):\n",
    "      data, doc_map = self.create_corpus(training_data)\n",
    "      cv = CountVectorizer()\n",
    "      data = cv.fit_transform(data)\n",
    "      tfidf_transformer = TfidfTransformer()\n",
    "      tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "      word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "      return word2tfidf\n",
    "\n",
    "    def token_to_tfidf(self , word2tfidf):\n",
    "      model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "      tmptokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      dic = {}\n",
    "      for x in word2tfidf :\n",
    "         dic[tmptokenizer.convert_tokens_to_ids(x)] = word2tfidf[x]\n",
    "      return dic\n",
    "\n",
    "    def predict(self, training_data, evaluation_data):\n",
    "        corpus, doc_map = self.create_corpus(training_data)\n",
    "        vec = self.vectorizer.fit_transform(corpus)\n",
    "\n",
    "        acurracy_set = {}\n",
    "        all_type_acurracy = 0\n",
    "        for query in tqdm.tqdm(evaluation_data):\n",
    "            y_hat = self.predict_query(vec, doc_map, query)\n",
    "            y = {}\n",
    "            for type in evaluation_data[query]:\n",
    "                r_type = set()\n",
    "                for r in evaluation_data[query][type]:\n",
    "                    r_type.add(r)\n",
    "                y[type] = r_type\n",
    "\n",
    "            found = False\n",
    "            for type in y:\n",
    "                intersect = y_hat.intersection(y[type])\n",
    "                if type not in acurracy_set:\n",
    "                    acurracy_set[type] = 0\n",
    "                if len(intersect) > 0:\n",
    "                    acurracy_set[type] += 1\n",
    "                    found = True\n",
    "\n",
    "            if found:\n",
    "                all_type_acurracy += 1\n",
    "\n",
    "        print()\n",
    "        for type in acurracy_set:\n",
    "            accuracy = round(100 * acurracy_set[type] / len(evaluation_data), 2)\n",
    "            print(f\"Accuracy For Type {type} = {accuracy}\")\n",
    "\n",
    "        print(f\"Accuracy For All Type = {round(100 * all_type_acurracy / len(evaluation_data), 2)}\")\n",
    "\n",
    "    def predict_query(self, vec, doc_map, query):\n",
    "        similarity = self.vectorizer.transform([query]).dot(vec.T)\n",
    "        r_doc_set = set()\n",
    "        for doc in self.largest_indices(similarity.toarray()[0], self.number)[0]:\n",
    "            r_doc_set.add(doc_map[doc].replace('.txt', ''))\n",
    "        return r_doc_set\n",
    "\n",
    "    def create_corpus(self, data):\n",
    "        corpus = []\n",
    "        doc_map = []\n",
    "        for filename in data:\n",
    "            x = data[filename]\n",
    "            corpus.append(x['content'])\n",
    "            doc_map.append(filename)\n",
    "        return corpus, doc_map\n",
    "\n",
    "    def largest_indices(self, array, number):\n",
    "        flat = array.flatten()\n",
    "        indices = np.argpartition(flat, -number)[-number:]\n",
    "        indices = indices[np.argsort(-flat[indices])]\n",
    "        return np.unravel_index(indices, array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'> قسمت دوم - bert فارسی</center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 16px'>\n",
    "  <ol>\n",
    "      <li>تابع (predict): اسناد را با bert جاسازی می‌کند و نتایج کویری را به دست آورده و دقت را با توجه به اطلاعات ارزیابی محاسبه می‌کند.</li>\n",
    "      <li>تابع (predict_query):بر اساس ماتریس شباهت که با روش بازنمایی برت تولید شده اندیس مستندات مربوطه را از ماتریس استخراج و سپس نام هریک از فایل های بازیابی شده را برای مقایسه در بخش ارزیابی ارائه می نماید  </li>\n",
    "      <li>تابع (extract_document_embeding_matrix):جاسازی تمام اسناد را در یک ماتریس ذخیره می‌کند </li>\n",
    "      <li>تابع (extract_query_embeding_matrix):نتایج جاسازی کویری‌ها را در یک ماتریس ذخیره می‌کند </li>\n",
    "      <li>تابع (get_document_embeding): به ازای هر سند جاسازی را استخراج می‌کند</li>\n",
    "      <li>تابع (split_sentences): اگر طول جملات بیشتر از مقدار مورد قبول برت یعنی ۵۱۲ بود، جملات را به قسمت‌های کوچکتر می‌شکند.</li>\n",
    "      <li>تابع (largest_indices): در صورتیکه در پارامتر ورودی مقدار n تعیین شود تعداد n اندیس آرایه با بیشترین مقدار را که به صورت نزولی مرتب شده است برمی گرداند.</li>\n",
    "\n",
    "  </ol> \n",
    "  </div>\n",
    "  <h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'> قسمت سوم ترکیبی</center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 16px'>\n",
    "همان قسمت دوم است که دیکشنری ساخته شده در tfidf را هم به عنوان ورودی می‌گیرد و با mode = 1 \n",
    "  اجرا می‌شود.\n",
    "  ضرایبی که از دیکشنری برای هر توکن پیدا می‌کند را در جاسازی برت ضرب می‌کند و به این ترتیب برای جاسازی سند از میانگین وزن‌دار جاسازی توکن‌ها استفاده می‌کند.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ren6WetDVbcz"
   },
   "outputs": [],
   "source": [
    "from transformers.file_utils import tf_required\n",
    "\n",
    "class ParsBertEmbeding:\n",
    "    model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "\n",
    "    def __init__(self, number):\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.number = number\n",
    "\n",
    "    def predict(self,  training_data, evaluation_data , mode , dic = None):\n",
    "        doc_emb_matrix, doc_filename_list = self.extract_document_embeding_matrix(training_data , mode , dic)\n",
    "        query_emb_matrix, query_list = self.extract_query_embeding_matrix(evaluation_data , mode , dic)\n",
    "        similarity_matrix = query_emb_matrix.dot(doc_emb_matrix.T)\n",
    "        acurracy_set = {}\n",
    "        all_type_acurracy = 0\n",
    "        for query_index in range(0, len(query_list)):\n",
    "            query = query_list[query_index]\n",
    "            y_hat = self.predict_query(similarity_matrix, doc_filename_list, query_index)\n",
    "            y = {}\n",
    "            for type in evaluation_data[query]:\n",
    "                r_type = set()\n",
    "                for r in evaluation_data[query][type]:\n",
    "                    r_type.add(r)\n",
    "                y[type] = r_type\n",
    "\n",
    "            found = False\n",
    "            for type in y:\n",
    "                intersect = y_hat.intersection(y[type])\n",
    "                if type not in acurracy_set:\n",
    "                    acurracy_set[type] = 0\n",
    "                if len(intersect) > 0:\n",
    "                    acurracy_set[type] += 1\n",
    "                    found = True\n",
    "\n",
    "            if found:\n",
    "                all_type_acurracy += 1\n",
    "\n",
    "        print()\n",
    "        for type in acurracy_set:\n",
    "            accuracy = round(100 * acurracy_set[type] / len(evaluation_data), 2)\n",
    "            print(f\"Accuracy For Type {type} = {accuracy}\")\n",
    "\n",
    "        print(f\"Accuracy For All Type = {round(100 * all_type_acurracy / len(evaluation_data), 2)}\")\n",
    "        return similarity_matrix\n",
    "\n",
    "    def predict_query(self, similarity_matrix, doc_filename_list, query_index):\n",
    "        r_doc_set = set()\n",
    "        for doc_index in self.largest_indices(similarity_matrix[query_index, :], self.number)[0]:\n",
    "            r_doc_set.add(doc_filename_list[doc_index].replace('.txt', ''))\n",
    "        return r_doc_set\n",
    "\n",
    "    def extract_document_embeding_matrix(self, data , mode , dic):\n",
    "        doc_emb_matrix = np.zeros((1, 768), dtype=float)\n",
    "        doc_filename_list = list()\n",
    "        for filename in tqdm.tqdm(data):\n",
    "            doc = data[filename]\n",
    "            doc_emb = self.get_document_embeding(doc , mode , dic)\n",
    "            doc_emb_matrix = np.concatenate((doc_emb_matrix, doc_emb), axis=0)\n",
    "            doc_filename_list.append(filename)\n",
    "\n",
    "        doc_emb_matrix = doc_emb_matrix[1:, :]\n",
    "        doc_emb_matrix_normed = normalize(doc_emb_matrix, axis=1, norm='l1')\n",
    "        return doc_emb_matrix_normed, doc_filename_list\n",
    "\n",
    "    def extract_query_embeding_matrix(self, evaluation_data , mode , dic):\n",
    "        query_emb_matrix = np.zeros((1, 768), dtype=float)\n",
    "        query_list = list()\n",
    "        for query in tqdm.tqdm(evaluation_data):\n",
    "            query_emb = torch.zeros((768), dtype=torch.float64)\n",
    "            query_tokens = self.tokenizer(query, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**query_tokens)\n",
    "                if mode == 1 :\n",
    "                  for x in range(len(query_tokens['input_ids'][0])):\n",
    "                    if query_tokens['input_ids'][0][x].item() in dic:\n",
    "                        outputs.last_hidden_state[0, :, :][x] = outputs.last_hidden_state[0, :, :][x]*dic[query_tokens['input_ids'][0][x].item()]\n",
    "                    else :\n",
    "                        outputs.last_hidden_state[0, :, :][x] = outputs.last_hidden_state[0, :, :][x]*0\n",
    "                query_emb += torch.mean(outputs.last_hidden_state[0, :, :], dim=0)\n",
    "\n",
    "            query_emb_matrix = np.concatenate((query_emb_matrix, query_emb[np.newaxis, :].numpy()), axis=0)\n",
    "            query_list.append(query)\n",
    "\n",
    "        query_emb_matrix = query_emb_matrix[1:, :]\n",
    "        query_emb_matrix_normed = normalize(query_emb_matrix, axis=1, norm='l1')\n",
    "        return query_emb_matrix_normed, query_list\n",
    "\n",
    "    def get_document_embeding(self, doc , mode , dic ):\n",
    "        sent_list = []\n",
    "        for sent in doc['sentences']:\n",
    "           sent_list.extend(self.split_sentences(sent))\n",
    "\n",
    "        emb_matrix = torch.zeros((1, 768), dtype=torch.float64)\n",
    "        for sent in sent_list:\n",
    "            sent_tokens = self.tokenizer(sent, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    outputs = self.model(**sent_tokens)\n",
    "                    if mode == 1 :\n",
    "                      for x in range(len(sent_tokens['input_ids'][0])):\n",
    "                       if sent_tokens['input_ids'][0][x].item() in dic:\n",
    "                        outputs.last_hidden_state[0, :, :][x] = outputs.last_hidden_state[0, :, :][x]*dic[sent_tokens['input_ids'][0][x].item()]\n",
    "                       else :\n",
    "                         outputs.last_hidden_state[0, :, :][x] = outputs.last_hidden_state[0, :, :][x]*0\n",
    "                    emb_matrix = torch.cat((emb_matrix, outputs.last_hidden_state[0, :, :]), axis=0)\n",
    "                except:\n",
    "                    print(\"An exception occurred\")\n",
    "\n",
    "        emb_matrix_avg = torch.mean(emb_matrix, dim=0)\n",
    "        return emb_matrix_avg[np.newaxis, :].numpy()\n",
    "\n",
    "    def split_sentences(self, sent):\n",
    "        word_in_sentence = 450\n",
    "        sent_list = []\n",
    "        word_token_list = word_tokenize(sent)\n",
    "        if len(word_token_list) < word_in_sentence:\n",
    "            sent_list.append(sent)\n",
    "            return sent_list\n",
    "\n",
    "        num_sent = math.ceil(len(word_token_list) / word_in_sentence)\n",
    "        start = 0\n",
    "        end = start + word_in_sentence\n",
    "        for index in range(0, num_sent):\n",
    "            sent_list.append(' '.join(word_token_list[start:end]))\n",
    "            start = end + 1\n",
    "            end = min(start + word_in_sentence, len(word_token_list))\n",
    "        return sent_list\n",
    "\n",
    "    def largest_indices(self, array, number):\n",
    "        flat = array.flatten()\n",
    "        indices = np.argpartition(flat, -number)[-number:]\n",
    "        indices = indices[np.argsort(-flat[indices])]\n",
    "        return np.unravel_index(indices, array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "فایل‌ها را از مسیر مورد نظر خوانده و اطلاعات را در training_data نگه داری می‌کند. هم چنین اطلاعات فایل ارزیابی را در evaluation_data نگه می‌دارد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16009,
     "status": "ok",
     "timestamp": 1673753140069,
     "user": {
      "displayName": "z fazli",
      "userId": "01348446294497098456"
     },
     "user_tz": -210
    },
    "id": "b-No52pN5P7n",
    "outputId": "3b4ff242-20d3-45bf-a47a-3bed4734fe5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3258/3258 [00:11<00:00, 280.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3258/3258 [00:06<00:00, 511.45it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = LoadAndPreprocessing()\n",
    "training_data = loader.load_documents()\n",
    "training_data = loader.tokenize_normalize_lemmatize(training_data)\n",
    "evaluation_data = loader.load_evaluation_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>نتایج قسمت اول </center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "مربوط به اجرای tfidf است. همان طور که از نتایج مشخص است این روش می‌تواند با دقت قابل قبولی روی اسناد داده شده جستجو را انجام دهد\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5020,
     "status": "ok",
     "timestamp": 1673753147354,
     "user": {
      "displayName": "z fazli",
      "userId": "01348446294497098456"
     },
     "user_tz": -210
    },
    "id": "URQAli0JSWYG",
    "outputId": "c52fea12-867c-4da3-c1ef-1fc5a44b5422"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "100%|██████████| 150/150 [00:04<00:00, 31.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy For Type relevant = 91.33\n",
      "Accuracy For Type similar_high = 90.67\n",
      "Accuracy For Type similar_low = 0.0\n",
      "Accuracy For Type similar_med = 84.0\n",
      "Accuracy For All Type = 96.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib.function_base import vectorize\n",
    "\n",
    "vectorizer = TfIdfVectorization(20)\n",
    "word_to_value_dic = vectorizer.set_tfidf_word(training_data)\n",
    "token_to_tfidf = vectorizer.token_to_tfidf(word_to_value_dic)\n",
    "vectorizer.predict(training_data, evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>نتایج قسمت دوم و سوم </center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "نتایج اول مربوط به اجرای الگوریتم با مود ۰ است که در واقع فقط از برت برای جاسازی اسناد استفاده می‌کند. همان طور که از نتایج مشخص است این قسمت نسبت\n",
    "به tfidf از دقت کمتری برخوردار است ۸۴ درصد به نسبت ۹۷ درصد در tfidf. زمان اجرا هم در این حالت به صورت چشمگیری نسبت به tfidf  بالاست.\n",
    "در این قسمت هر سند به صورت جمله به جمله به برت داده می‌شود و برای به دست آوردن جاسازی سند میانگین جاسازی تمام توکن‌ها به عنوان جاسازی سند در نظر گرفته می‌شود. \n",
    "وقتی به جای میانگین با کمک tfidf میانگین وزن‌دار می‌گیریم، دقت کار افزایش پیدا می‌کند (ار ۸۴ درصد به ۹۲ درصد افزایش می‌یابد)و علت آن هم این است که تمام کلمات موجود در متن ارزش یکسانی ندارند و با دادن ضریب به کلمات مهم‌تر، نتیجه‌ی دقیق‌تری تولید می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8KvOAFwSayo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 17%|█▋        | 548/3258 [43:38<5:40:36,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 549/3258 [43:47<5:59:21,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 570/3258 [45:13<2:38:41,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 573/3258 [45:18<1:57:29,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 578/3258 [45:31<1:36:25,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 1351/3258 [1:47:24<2:44:40,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1522/3258 [1:59:58<1:49:33,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2051/3258 [2:40:15<1:18:00,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 2069/3258 [2:41:31<1:14:38,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2338/3258 [3:05:28<1:04:48,  4.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2571/3258 [3:24:53<42:34,  3.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2615/3258 [3:28:40<42:01,  3.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 2774/3258 [3:40:11<26:11,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 3236/3258 [4:16:45<02:03,  5.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3258/3258 [4:18:52<00:00,  4.77s/it]\n",
      "100%|██████████| 150/150 [00:21<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy For Type relevant = 48.67\n",
      "Accuracy For Type similar_high = 70.67\n",
      "Accuracy For Type similar_low = 4.67\n",
      "Accuracy For Type similar_med = 61.33\n",
      "Accuracy For All Type = 84.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 548/3258 [48:09<6:17:55,  8.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 549/3258 [48:18<6:18:48,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 570/3258 [49:53<2:55:53,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 573/3258 [49:59<2:09:27,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 577/3258 [50:13<2:27:25,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 1351/3258 [1:59:23<3:03:21,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1522/3258 [2:13:31<2:03:06,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2051/3258 [2:58:32<1:27:57,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 2069/3258 [3:00:00<1:28:29,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2338/3258 [3:26:51<1:14:57,  4.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2571/3258 [3:47:59<48:43,  4.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2615/3258 [3:52:04<46:43,  4.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 2774/3258 [4:04:53<29:14,  3.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 3236/3258 [4:41:52<01:54,  5.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3258/3258 [4:43:48<00:00,  5.23s/it]\n",
      "100%|██████████| 150/150 [00:19<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy For Type relevant = 58.67\n",
      "Accuracy For Type similar_high = 77.33\n",
      "Accuracy For Type similar_low = 3.33\n",
      "Accuracy For Type similar_med = 72.67\n",
      "Accuracy For All Type = 92.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00172171, 0.00158113, 0.00152524, ..., 0.0014316 , 0.00141329,\n",
       "        0.00159319],\n",
       "       [0.00135551, 0.00127134, 0.0012751 , ..., 0.00132523, 0.00128545,\n",
       "        0.00150147],\n",
       "       [0.00160676, 0.00146611, 0.00140225, ..., 0.0018339 , 0.00180934,\n",
       "        0.00182751],\n",
       "       ...,\n",
       "       [0.00133119, 0.00126054, 0.00123567, ..., 0.00134421, 0.00131315,\n",
       "        0.0014227 ],\n",
       "       [0.00147329, 0.00135558, 0.00139093, ..., 0.00152353, 0.00150857,\n",
       "        0.00148624],\n",
       "       [0.0013731 , 0.00125534, 0.00124769, ..., 0.00135081, 0.00133622,\n",
       "        0.00135603]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = ParsBertEmbeding(20)\n",
    "bert.predict(training_data, evaluation_data , 0)\n",
    "bert.predict(training_data, evaluation_data  , 1 , token_to_tfidf )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>قسمت امتیازی - طبقه بندی</center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "در این بخش هدف این است تا در ابتدا به وسیله امبدینگ خروجی برت فارسی، ابتدا مجموعه داده ای از زوج های (پرسمان مرتبط - سند) و (پرسمان نامرتبط - سند) ایجاد کنیم و سپس با استفاده از این مجموعه داده یک دسته بند ایجاد کنیم و بتوانیم دادگان را از منظر گفته شده طبقه بندی کنیم. برای بخش ساخت مجموعه داده به کلاس پارس-برت-امبدینگ دو تابع اضافه شده است که در ادامه توضیحات آن ها آورده شده است. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 16px'>\n",
    "  <ol>\n",
    "      <li>تابع (extract_both_embeding_matrices): این تابع ماتریس امدبینگ سند و کوئری ها را  مطابق با توضیحات قسمت های قبل استخراج کرده و برای استفاده در تابع بعدی به عنوان خروجی میدهد. </li>\n",
    "      <li>تابع (making_dataset): این تابع داده ست های ذکر شده را استحراج میکند. روال کار به این صورت است که در کل با توجه به فایل ارزیابی داده شده، دو بردار خروجی تولید میکند، یکی بردار ایکس و دیگری بردار ایگرگ. هر سطر از بردار ایکس کانکت شده امبدینگ یک سند و یک کوئری است که این امبدینگ ها خروجی پارس برت هستند. به همین ترتیب در هر سطر متناظر بردار ایگرگ یک عدد برای مشخص کردن این که سند و کوئری هر سطر ایکس با یکدیگر مرتبط هستند یا خیر قرار میگیرد. لازم به ذکر است که برای هر کوئری 70 سند در ایکس قرار گرفته اند که از این 70 تا سند، تعدادی سند های مرتبط اند که از فایل ارزیابی استخراج شده اند و باقی مانده ان ها به صورت تصادفی از بقیه سند های نامرتبط انتخاب شده اند.     </li>\n",
    "  </ol> \n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "در ادامه باری دیگر کلاس پارس پرت امبدینگ صرفا با تابع های مورد نیاز این بخش اورده شده است.البته میتوان این تابع ها را به کلاس اصلی اضافه کرده و نیاز به بازنویسی مجدد نباشد، اما برای شفاف تر بودن طریقه تولید دیتاست بار دیگر کلاس آورده شده است. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsBertEmbeding_c:\n",
    "    model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "    \n",
    "    def extract_both_embeding_matrices(self, training_data, evaluation_data):\n",
    "        doc_emb_matrix, doc_filename_list = self.extract_document_embeding_matrix(training_data)\n",
    "        query_emb_matrix, query_list = self.extract_query_embeding_matrix(evaluation_data)\n",
    "        \n",
    "        return doc_emb_matrix, doc_filename_list, query_emb_matrix, query_list\n",
    "    \n",
    "    def making_dataset(self, doc_emb_matrix, query_emb_matrix, doc_filename_list, query_list, evaluation_data):\n",
    "        \n",
    "        doc_filename_list = [int(w.replace(\".txt\",\"\")) for w in doc_filename_list]\n",
    "        \n",
    "        number_of_docs = len(doc_filename_list)\n",
    "        number_of_queries = len(query_list)\n",
    "        \n",
    "        each_query_no = 70\n",
    "        embeding_length = len(doc_emb_matrix[0])\n",
    "        columns = 2 * embeding_length\n",
    "        rows = each_query_no * number_of_queries\n",
    "        \n",
    "        x_vector = np.zeros((rows,columns))\n",
    "        y_vector = np.zeros((rows,1))\n",
    "            \n",
    "        # for each query, we get the relevant, similar_high, similar_med-> relevant and assign 1 for their y \n",
    "        # for each query, we produce 70 data for our training data, some from above, some random from rest\n",
    "        over_all_counter = 0\n",
    "        for query in query_list:\n",
    "            counter = 0\n",
    "            pre_doc = []\n",
    "            for doc in evaluation_data[query]['relevant']:\n",
    "                doc_name = int(doc)\n",
    "                doc_index = doc_filename_list.index(doc_name)\n",
    "                pre_doc.append(doc_index)\n",
    "                query_index = query_list.index(query)\n",
    "                mixed_embedding = np.concatenate((doc_emb_matrix[doc_index],query_emb_matrix[query_index]))\n",
    "                x_vector[over_all_counter] = mixed_embedding\n",
    "                y_vector[over_all_counter] = 1\n",
    "                counter += 1\n",
    "                over_all_counter += 1\n",
    "\n",
    "            for doc in evaluation_data[query]['similar_high']:\n",
    "                doc_name = int(doc)\n",
    "                doc_index = doc_filename_list.index(doc_name)\n",
    "                pre_doc.append(doc_index)\n",
    "                query_index = query_list.index(query)\n",
    "                mixed_embedding = np.concatenate((doc_emb_matrix[doc_index],query_emb_matrix[query_index]))\n",
    "                x_vector[over_all_counter] = mixed_embedding\n",
    "                y_vector[over_all_counter] = 1\n",
    "                counter += 1\n",
    "                over_all_counter += 1\n",
    "\n",
    "            for doc in evaluation_data[query]['similar_med']:\n",
    "                doc_name = int(doc)\n",
    "                doc_index = doc_filename_list.index(doc_name)\n",
    "                pre_doc.append(doc_index)\n",
    "                query_index = query_list.index(query)\n",
    "                mixed_embedding = np.concatenate((doc_emb_matrix[doc_index],query_emb_matrix[query_index]))\n",
    "                x_vector[over_all_counter] = mixed_embedding\n",
    "                y_vector[over_all_counter] = 1\n",
    "                counter += 1\n",
    "                over_all_counter += 1\n",
    "            \n",
    "            while counter < each_query_no:\n",
    "                random_index = rand.randint(0, number_of_docs-1)\n",
    "                if random_index not in pre_doc:\n",
    "                    doc_index = random_index\n",
    "                    pre_doc.append(doc_index)\n",
    "                    query_index = query_list.index(query)\n",
    "                    mixed_embedding = np.concatenate((doc_emb_matrix[doc_index],query_emb_matrix[query_index]))\n",
    "                    x_vector[over_all_counter] = mixed_embedding\n",
    "                    y_vector[over_all_counter] = 0\n",
    "                    counter += 1\n",
    "                    over_all_counter += 1\n",
    "    \n",
    "        return x_vector, y_vector\n",
    "            \n",
    "            \n",
    "    def extract_document_embeding_matrix(self, data):\n",
    "        doc_emb_matrix = np.zeros((1, 768), dtype=float)\n",
    "        doc_filename_list = list()\n",
    "        for filename in tqdm.tqdm(data):\n",
    "            doc = data[filename]\n",
    "            doc_emb = self.get_document_embeding(doc)\n",
    "            doc_emb_matrix = np.concatenate((doc_emb_matrix, doc_emb), axis=0)\n",
    "            doc_filename_list.append(filename)\n",
    "\n",
    "        doc_emb_matrix = doc_emb_matrix[1:, :]\n",
    "        doc_emb_matrix_normed = normalize(doc_emb_matrix, axis=1, norm='l1')\n",
    "        return doc_emb_matrix_normed, doc_filename_list\n",
    "\n",
    "    def extract_query_embeding_matrix(self, evaluation_data):\n",
    "        query_emb_matrix = np.zeros((1, 768), dtype=float)\n",
    "        query_list = list()\n",
    "        for query in tqdm.tqdm(evaluation_data):\n",
    "            query_emb = torch.zeros((768), dtype=torch.float64)\n",
    "            query_tokens = self.tokenizer(query, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**query_tokens)\n",
    "                query_emb += torch.mean(outputs.last_hidden_state[0, :, :], dim=0)\n",
    "\n",
    "            query_emb_matrix = np.concatenate((query_emb_matrix, query_emb[np.newaxis, :].numpy()), axis=0)\n",
    "            query_list.append(query)\n",
    "\n",
    "        query_emb_matrix = query_emb_matrix[1:, :]\n",
    "        query_emb_matrix_normed = normalize(query_emb_matrix, axis=1, norm='l1')\n",
    "        return query_emb_matrix_normed, query_list\n",
    "\n",
    "    def get_document_embeding(self, doc):\n",
    "        sent_list = []\n",
    "        for sent in doc['sentences']:\n",
    "            sent_list.extend(self.split_sentences(sent))\n",
    "\n",
    "        emb_matrix = torch.zeros((1, 768), dtype=torch.float64)\n",
    "        for sent in sent_list:\n",
    "            sent_tokens = self.tokenizer(sent, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    outputs = self.model(**sent_tokens)\n",
    "                    emb_matrix = torch.cat((emb_matrix, outputs.last_hidden_state[0, :, :]), axis=0)\n",
    "                except:\n",
    "                    print(\"An exception occurred\")\n",
    "\n",
    "        emb_matrix_avg = torch.mean(emb_matrix, dim=0)\n",
    "        return emb_matrix_avg[np.newaxis, :].numpy()\n",
    "\n",
    "    def split_sentences(self, sent):\n",
    "        word_in_sentence = 450\n",
    "        sent_list = []\n",
    "        word_token_list = word_tokenize(sent)\n",
    "        if len(word_token_list) < word_in_sentence:\n",
    "            sent_list.append(sent)\n",
    "            return sent_list\n",
    "\n",
    "        num_sent = math.ceil(len(word_token_list) / word_in_sentence)\n",
    "        start = 0\n",
    "        end = start + word_in_sentence\n",
    "        for index in range(0, num_sent):\n",
    "            sent_list.append(' '.join(word_token_list[start:end]))\n",
    "            start = end + 1\n",
    "            end = min(start + word_in_sentence, len(word_token_list))\n",
    "        return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 17%|████████████▉                                                                | 548/3258 [09:46<1:18:29,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 17%|████████████▉                                                                | 549/3258 [09:48<1:24:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                 | 570/3258 [10:07<33:58,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▉                                                                 | 573/3258 [10:08<24:30,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▉                                                                 | 577/3258 [10:11<28:20,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████████████████████▎                                             | 1351/3258 [23:54<40:47,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████▍                                         | 1522/3258 [26:45<24:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████                             | 2051/3258 [35:11<16:25,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████▌                            | 2069/3258 [35:27<16:30,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████▉                      | 2338/3258 [40:45<14:12,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████▌                | 2571/3258 [44:56<10:21,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▌               | 2615/3258 [45:48<09:39,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▍           | 2774/3258 [48:13<05:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 3236/3258 [55:30<00:23,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred\n",
      "An exception occurred\n",
      "An exception occurred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3258/3258 [55:56<00:00,  1.03s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:06<00:00, 22.79it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_c = ParsBertEmbeding_c()\n",
    "doc_emb_matrix, doc_filename_list, query_emb_matrix, query_list = bert_c.extract_both_embeding_matrices(training_data, evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataset, y_dataset = bert_c.making_dataset(doc_emb_matrix, query_emb_matrix, doc_filename_list, query_list, evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='direction:rtl; font-family: \"B Lotus\";'> \n",
    "    <center style ='font-family: \"B Lotus\";'>ادامه قسمت امتیازی و انجام عملیات طبقه بندی</center> \n",
    "</h1> \n",
    "<div style='direction:rtl; font-family: \"B Lotus\"; font-size: 18px'>\n",
    "در این قسمت با استفاده از دیتاست های استخراج شده در مرحله قبل و با استفاده از لاجیستیک رگرسیون یک طبقه بندی بر روی داده ها صورت گرفته است که در نهایت به امتیاز 79 درصد داده ها را برای ما طبقه بندی میکند. لازم به ذکر است که از دیتاست تولید شده در مرحله قبل 80 درصد برای آموزش لاجیستیک رگرسیون و 20 درصد برای تست مدل آموزش دیده استفاده شده است.\n",
    "کد و خروجی مرتبط با این قسمت در ادامه آورده شده است. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dataset, y_dataset, random_state=0, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score is :0.7880952380952381\n"
     ]
    }
   ],
   "source": [
    "LR_classifier = LogisticRegression()\n",
    "LR_classifier.fit(x_train, np.ravel(y_train))\n",
    "print(\"Classification score is :\" + str(LR_classifier.score(x_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJJ9Dkew4DEG3wqfWdB0j4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dee524ac1bdec8140fa2f9ea215b9134edb69c686e38c4b7302051d1098d2cf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
